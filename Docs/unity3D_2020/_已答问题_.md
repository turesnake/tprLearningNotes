# ================================================================ #
#                  unity3d  已答问题
#                 （待做清单 的完成部分）
# ================================================================ #
相当一部分问题片段 已经丢失...


# --- 015 --- #
# 什么叫做 lobe ？？？ 
# :
    波瓣


# --- 024 --- #
# urp 中，是如何实现 normaldir tangent-space to WS 转换的 ？
# :
    和 fenglele 书中的实现一样：
        float3x3 mat = float3x3( tangentWS, binormalWS, normalWS );
        float3 out = mul( in_, mat );
    注意，在这个实现中，装配的 矩阵， 是目标矩阵的 转置矩阵
    因此，第二步的 mul() 计算中，使用的是 左乘



# --- 029 --- #
# unity 中， z-buffer 取值范围到底是多少
# 它和 depth buffer 之间的转化是怎么样的 
# :
    z-buffer 就是 depth buffer
    是一个非线性的 [0,1] 区间，
    除 opengl 以外的 大部分平台，都已经支持 reverseZBuffer
    即：
        0: far
        1: near

# --- 035 --- #
# 如何在 urp 中实现 z-buffer show ???
# :
    需要事先准备好 depthOnly pass,
    以便把数据，填入 _CameraColorTexture


# --- 040 --- #
# 什么是 G-Buffer (geometry buffer)
# :
    builtin manual 将 camera depth, depth+normals, or motion vector Texture
    看作是 最简单的 G-Buffer texture
#
    GBuffer 被认为 和 Deferred shading 有关
#
    The G-buffer is the collective term of all textures used to store lighting-relevant data 
    for the final lighting pass.



# --- 041 --- #
# unity 支持 MRT (multiple render targets) 吗 ？
# :
    builtin,urp 可以手动支持
    hdrp 因为是 deferred shading，所以天生支持 mrt



# --- 043 --- #
# Deferred shading
# 缺点：
- 因为依赖 z-buffer, 所以不擅长表现 半透明物体
    现代硬件似乎有对策
- 不适合 multiple materials
    这会增加显存开销
- 因为将 geometric 和 lighting 分离了，基于硬件的 anti-aliasing 不再表现良好
    但是似乎存在新技术


# --- 045 --- #
# urp  是否支持 forward / deferred rendering
# hdrp 是否支持 forward / deferred rendering
# :
    urp 支持 forward rendering
    urp 的 deferred rendering 还在开发中

    hdrp 同时支持 forward / deferred rendering
    hdrp 不光都支持，还支持运行时 在两种 模式之间切换



# --- 047 --- #
# 什么是 _MainTex_TexelSize ???
# :
# float4 _MainTex_TexelSize;
    假设 texture 的长宽为 (w,h) [pix]
    此值为 ( 1/w, 1/h, w, h )
---
[此段存疑]
    y is negative when it belongs to a RenderTexture that has 
    been flipped vertically by D3D anti-aliasing.
    ---
    在官方 munaul 中，也没提到这段



# --- 048 --- #
# shader_feature 与 multi_compile 的区别 
# :
    manual: Shader variants and keywords
#
    两种格式的最终目的 都是为了生成 关键词 / keywords
        以此来达成 宏分支，最终生成不同的 shader 变体
        毕竟在 shader 中，分支语句是十分昂贵的
        --
    进一步的，也可以在运行时 修改 keyword 的状态：enable/disable
        以此来在不同的 shader variants 之间切换

# #pragma multi_compile Sml Mid Big 
    unity 查找 这三个 keywords 是否在 cs文件中被 enable
    如果一个都没被 enable，就启用第一个
    （所以它们不能被分开写成数行）

# multi_compile 中 "_" 的意义
    在原始实现中，如果我们想要实现一个 开关 功能，我们需要这样做
        #pragma multi_compile AOn AOff
    要么 enable AOn keyword，要么 enable AOff keyword
    这会造成 keywords 数量的膨胀，
    而 unity shader 中，keywords 数量是有上限的
    ---
    一种对策就是：
        #pragma multi_compile _ AOn
    在这种实现中，只存在一个 keyword，如果 AOn 未被 enable
    则 unity 自动 enable "_", 而这个 keyword 是 “功能上无意义的”
    也就是说， "_" 虽然是一个正确的 keyword，但我们不会在 代码中用到它
    从而实现 节省 keyword 的目的

# shader_feature 与 multi_compile 的区别 
    The only difference is that Unity does not include 
    unused variants of shader_feature shaders in the final build. 
---
    For this reason, you should use shader_feature for keywords that are set from the Materials, 
    while multi_compile is better for keywords that are set from code globally.
---
    shader_feature 可以认为是 multi_complie 的子集，其与 multi_complie 最大的不同就是
    此关键字的声明变体是材质球层级的（multi_complie是全局），只能通过美术在制作时调整相应材质，
    未被选择的变体会在打包的时候被舍弃（multi_complie不会），所以其声明的变体是不能通过代码控制的（打包后会出问题）

# 
        #pragma shader_feature A
    等于 #pragma shader_feature _ A
    这只是一种简写
        ---
        但是，这个简写，只在 本行只有一个 keyword 时才有效
        当有数个 keywords 时，就会自动默认 第一个 keyword 是 enable 的

# local keyword 可以由谁来改 ？
    mat.EnableKeyword();
    使用其它方式 只能修改 global keywords, 不会起作用 


# --- 059 --- #
# 什么是: screen space derivatives of world space position
# 什么是: ddx, ddy, fwidth
#
    gpu 在一个时间段里，会同时给一组 pix 上色
    they are always running 32-64 pixels at the same time using a SIMD architecture

    在那篇 2013 年的网页中，称 gpu 会将 2X2 个 pix 当作一个整体来处理，ddx，ddy 似乎是用来修正每个pix的偏移的
#
    当代 GPGPU 在像素化的时候一般是以2x2像素为基本单位，那么在这个2x2像素块当中，
    右侧的像素对应的 fragment 的x坐标减去左侧的像素对应的 fragment 的x坐标就是 ddx；
    下侧像素对应的 fragment 的坐标y减去上侧像素对应的 fragment 的坐标y就是 ddy。

    ddx 和 ddy 代表了相邻两个像素在设备坐标系当中的距离，
    据此可以判断应该使用哪一层的贴图LOD
    这个距离越大表示三角形离开摄像机越远，需要使用更小分辨率的贴图；
    反之表示离开摄像机近，需要使用更高分辨率的贴图。

# 这个东西 怎么越看越复杂...

# 
    fwidth = abs(ddx(x)) + abs(ddy(x));

#
    ddx（param）就是屏幕水平方向上一像素跨度内 参数属性param 的值改变了多少。
    所以，param 其实可以传入各种类型的值

#
    they are fragment shader instructions wich can be used to 
    compute the rate of variation of any value with respect to 
    the screen-space coordinates.

    实际计算的时候：
        ddx(v) = 该像素点右边的v值 - 该像素点的v值
        ddy(v) = 该像素点下面的v值 - 该像素点的v值

    虽然名为 偏导数，但计算方式却很简单...

#
    同一个 2X2 区间内的所有 pix，共享同一份 ddx,ddy 值


# --- 061 --- #
# 通过 mod(pos)， 可以在空间中渲染无数个整齐排列的 mesh
# 问题来了：
# 在这个场景中，对于单个 frag 的检测，它需要针对多少个 mesh 进行 检测？
#
    代码：
    float repeatedCubes(vec3 p) {
        p.x = mod(p.x, 1) - 0.5;
        p.z = mod(p.z, 1) - 0.5;
        return cube(p);
    }
    每一次检测，它只需要检测一个 mesh ！！！
    由带么可知：晶格的长度是 1，任何一个检测点 p，一定只会和自己晶格的 cube 最接近
    所以，只要计算一次，获得 t，然后步进即可。
#
    但是，这个算法的渲染速度仍然是缓慢的：
        每一个像素点在命中最终 cube 之前，走得都不会很快，
        这意味着，步进速度会退化为 constant speed
        进而导致渲染变慢


# --- 064 --- #
    什么是 Directional derivative
#
    函数 f(x,y) 在 x 轴上的 偏导数，其实就是 函数在方向 (1,0) 上的 方向导数
    计算公式:
        Df(x,y) = dot( 梯度向量, 方向V );
        Df(x,y,z) = dot( 梯度向量, 方向V );
        ---
        由 dot 运算可知，任何维度函数的 方向导数，一定是个一维值。
#
    对于 f(x,y) 而言，最终生成的结果，是一个 一维值（在方向V 上的导数）
    对于 f(x,y,z) 而言，最终生成的结果，也应该是一个 一维值 
# 快速计算法
    按照公式，最正式的计算当然是先算出 梯度，然后做 dot 运算。
    但是如果 梯度 计算本身就能麻烦，比如使用 central diffence method
    (意味着要做 4～6 次 采样)
    那么还存在一种更快速的，计算 方向导数 的方法：
    只采样 2 次：
    原始位置 采样一次，再沿着 方向V 前进一个微小距离 eps,然后再采样一次
    就能立刻求出 方向导数
# 方向导数 == 光照强度
    两者的公式是完全一样的: dot( normal, lightDir );
    所以，如果我们只是想 简单地计算 光照强度，
    那么完全没必要 计算 梯度（法线），然后再dot
    而是直接用上文的 快速计算法，计算最终值
    ---
    参考 iq 文:
    https://iquilezles.org/www/articles/derivative/derivative.htm
















    

